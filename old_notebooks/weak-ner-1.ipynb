{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a4a36-ae28-43d4-a6ef-44f67b47f204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 59 labels: ['O', 'B-Anwendung', 'B-Anzahl_Der_Einheiten', 'B-Besonderheiten', 'B-Breite', 'B-Bremsscheiben-Aussendurchmesser', 'B-Bremsscheibenart', 'B-Einbauposition', 'B-Farbe', 'B-Größe']...\n",
      "Gazetteers created for 29 tags\n",
      "  Kompatible_Fahrzeug_Marke: 173 entries\n",
      "  Kompatibles_Fahrzeug_Modell: 1902 entries\n",
      "  Herstellernummer: 1084 entries\n",
      "  Produktart: 196 entries\n",
      "  Im_Lieferumfang_Enthalten: 340 entries\n",
      "  Hersteller: 155 entries\n",
      "  Modell: 40 entries\n",
      "  Einbauposition: 41 entries\n",
      "  Bremsscheiben-Aussendurchmesser: 345 entries\n",
      "  Bremsscheibenart: 45 entries\n",
      "  Oe/Oem_Referenznummer(N): 228 entries\n",
      "  Maßeinheit: 10 entries\n",
      "  Anzahl_Der_Einheiten: 17 entries\n",
      "  Kompatibles_Fahrzeug_Jahr: 183 entries\n",
      "  Produktlinie: 4 entries\n",
      "  Material: 5 entries\n",
      "  Größe: 11 entries\n",
      "  Länge: 3 entries\n",
      "  Breite: 3 entries\n",
      "  Besonderheiten: 31 entries\n",
      "  Menge: 11 entries\n",
      "  Farbe: 1 entries\n",
      "  Stärke: 10 entries\n",
      "  Anwendung: 14 entries\n",
      "  Oberflächenbeschaffenheit: 2 entries\n",
      "  SAE_Viskosität: 2 entries\n",
      "  Zähnezahl: 3 entries\n",
      "  Technologie: 1 entries\n",
      "  Herstellungsland_Und_-Region: 1 entries\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Tuple, Set\n",
    "import pandas as pd, numpy as np, torch, torch.nn as nn\n",
    "from datasets import Dataset, DatasetDict\n",
    "from collections import defaultdict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig,\n",
    "    Trainer, TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "# ============================================================================\n",
    "# GAZETTEER CREATION - Extract from your 5K training data\n",
    "# ============================================================================\n",
    "\n",
    "def build_gazetteers(tagged_df):\n",
    "    \"\"\"Extract known entities from training data.\"\"\"\n",
    "    \n",
    "    gazetteers = {}\n",
    "    \n",
    "    for tag in tagged_df[\"Tag\"].unique():\n",
    "        if tag and tag != \"O\":\n",
    "            values = set(\n",
    "                tagged_df[tagged_df[\"Tag\"] == tag][\"Token\"]\n",
    "                .str.lower()\n",
    "                .str.strip()\n",
    "                .unique()\n",
    "            )\n",
    "            # Remove very common/short tokens (noise)\n",
    "            gazetteers[tag] = {v for v in values if len(v) > 0}\n",
    "    \n",
    "    return gazetteers\n",
    "\n",
    "\n",
    "# Load tagged data to get all possible tags\n",
    "tagged = pd.read_csv(\n",
    "    \"../data/Tagged_Titles_Train.tsv\",\n",
    "    sep=\"\\t\", keep_default_na=False, na_values=None\n",
    ")\n",
    "\n",
    "\n",
    "# Create BIO label list\n",
    "BASE = set(t for t in tagged[\"Tag\"].unique() if t and t != \"O\")\n",
    "label_list = [\"O\"] + sorted(\n",
    "    f\"{p}-{t}\" for t in BASE for p in (\"B\", \"I\")\n",
    ")\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "print(f\"Created {len(label_list)} labels: {label_list[:10]}...\")\n",
    "\n",
    "gazetteers = build_gazetteers(tagged)\n",
    "\n",
    "print(f\"Gazetteers created for {len(gazetteers)} tags\")\n",
    "for tag, values in gazetteers.items():\n",
    "    print(f\"  {tag}: {len(values)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4b5b42-b055-425b-bb2f-5ca8c1cd41cd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating weak labels using 16 workers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbbfbefff1442dfb0f3fe166179bf2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Weak labeling (parallel):   0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 199229 weakly-labeled examples\n",
      "✗ Skipped 771 examples\n",
      "  Coverage: 99.6%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# WEAK LABELING - Rule-based NER\n",
    "# ============================================================================\n",
    "from tqdm.auto import tqdm\n",
    "class WeakNERLabeler:\n",
    "    \"\"\"Generate weak NER labels using rules and gazetteers.\"\"\"\n",
    "    \n",
    "    def __init__(self, gazetteers: dict, category: int):\n",
    "        self.gazetteers = gazetteers\n",
    "        self.category = category\n",
    "        \n",
    "        # Automotive-specific patterns (German)\n",
    "        self.patterns = {\n",
    "            'Anzahl_Der_Einheiten': [\n",
    "                (r'\\b(\\d+)\\s*(stück|stk|x|pcs|piece)\\b', 1),\n",
    "                (r'\\b(\\d+)er\\s+set\\b', 1),\n",
    "            ],\n",
    "            'Einbauposition': [\n",
    "                (r'\\b(vorne?|hinten?|links?|rechts?|va|ha|vl|vr|hl|hr)\\b', 0),\n",
    "            ],\n",
    "            'Kompatible_Fahrzeug_Marke': [\n",
    "                # Will use gazetteer primarily\n",
    "            ],\n",
    "            'Kompatibles_Fahrzeug_Modell': [\n",
    "                # Common model patterns\n",
    "                (r'\\b([a-z]\\d+)\\b', 0),  # A3, X5, etc.\n",
    "            ],\n",
    "        }\n",
    "    \n",
    "    def label_text(self, text: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Returns list of (token, tag) pairs.\n",
    "        Tags are in BIO format.\n",
    "        \"\"\"\n",
    "        tokens = text.split()\n",
    "        labels = ['O'] * len(tokens)\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # 1. Gazetteer matching (exact + fuzzy)\n",
    "        for tag, entities in self.gazetteers.items():\n",
    "            for entity in entities:\n",
    "                entity_lower = entity.lower()\n",
    "                \n",
    "                # Multi-token entities\n",
    "                if ' ' in entity_lower:\n",
    "                    entity_tokens = entity_lower.split()\n",
    "                    for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "                        window = ' '.join(tokens[i:i+len(entity_tokens)]).lower()\n",
    "                        if window == entity_lower:\n",
    "                            labels[i] = f'B-{tag}'\n",
    "                            for j in range(i+1, i+len(entity_tokens)):\n",
    "                                labels[j] = f'I-{tag}'\n",
    "                else:\n",
    "                    # Single token\n",
    "                    for i, tok in enumerate(tokens):\n",
    "                        if tok.lower() == entity_lower and labels[i] == 'O':\n",
    "                            labels[i] = f'B-{tag}'\n",
    "        \n",
    "        # 2. Pattern matching\n",
    "        for tag, patterns in self.patterns.items():\n",
    "            for pattern, group_idx in patterns:\n",
    "                for match in re.finditer(pattern, text_lower):\n",
    "                    matched_text = match.group(group_idx)\n",
    "                    # Find token positions\n",
    "                    start_char = match.start(group_idx)\n",
    "                    end_char = match.end(group_idx)\n",
    "                    \n",
    "                    # Map character positions to token indices\n",
    "                    char_pos = 0\n",
    "                    for i, tok in enumerate(tokens):\n",
    "                        if char_pos <= start_char < char_pos + len(tok):\n",
    "                            if labels[i] == 'O':  # Don't override\n",
    "                                labels[i] = f'B-{tag}'\n",
    "                        char_pos += len(tok) + 1  # +1 for space\n",
    "        \n",
    "        return list(zip(tokens, labels))\n",
    "    \n",
    "    def label_batch(self, texts: List[str]) -> List[List[Tuple[str, str]]]:\n",
    "        \"\"\"Label multiple texts.\"\"\"\n",
    "        return [self.label_text(text) for text in texts]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# APPLY WEAK LABELING TO UNLABELED DATA\n",
    "# ============================================================================\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "def label_single_example(row_data, gazetteers, label2id, valid_tags):\n",
    "    \"\"\"Label a single example (for parallel processing).\"\"\"\n",
    "    idx, text, category = row_data\n",
    "    \n",
    "    if not text.strip():\n",
    "        return None\n",
    "    \n",
    "    labeler = WeakNERLabeler(gazetteers, category)\n",
    "    \n",
    "    try:\n",
    "        token_label_pairs = labeler.label_text(text)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    tokens = [tok for tok, _ in token_label_pairs]\n",
    "    labels = [label for _, label in token_label_pairs]\n",
    "    \n",
    "    # ✅ ADDED: Validate labels exist in label2id, otherwise use 'O'\n",
    "    label_ids = []\n",
    "    for lab in labels:\n",
    "        if lab in label2id:\n",
    "            label_ids.append(label2id[lab])\n",
    "        else:\n",
    "            # Invalid label - default to O\n",
    "            label_ids.append(label2id['O'])\n",
    "    \n",
    "    # Skip if no entities (after validation)\n",
    "    if all(lid == label2id['O'] for lid in label_ids):\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": label_ids,\n",
    "        \"Category\": category\n",
    "    }\n",
    "\n",
    "\n",
    "def create_weak_ner_dataset_parallel(\n",
    "    unlabeled_df: pd.DataFrame,\n",
    "    gazetteers: dict,\n",
    "    label2id: dict,\n",
    "    sample_size: int = 150000,\n",
    "    n_workers: int = None\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    Parallel version - much faster.\n",
    "    \"\"\"\n",
    "    df = unlabeled_df.sample(n=min(sample_size, len(unlabeled_df)), random_state=42)\n",
    "    \n",
    "    # Prepare data for parallel processing\n",
    "    row_data = [\n",
    "        (idx, str(row.get(\"Title\", \"\")), int(row.get(\"Category\", 1)))\n",
    "        for idx, row in df.iterrows()\n",
    "    ]\n",
    "    \n",
    "    if n_workers is None:\n",
    "        n_workers = max(1, cpu_count() - 2)  # Leave 2 cores free\n",
    "    \n",
    "    print(f\"Generating weak labels using {n_workers} workers...\")\n",
    "    \n",
    "    # Parallel processing\n",
    "    # Update partial function to include valid_tags\n",
    "    label_func = partial(\n",
    "        label_single_example, \n",
    "        gazetteers=gazetteers, \n",
    "        label2id=label2id,\n",
    "        valid_tags=BASE  # ✅ ADDED\n",
    "    )\n",
    "    \n",
    "    with Pool(n_workers) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(label_func, row_data, chunksize=100),\n",
    "            total=len(row_data),\n",
    "            desc=\"Weak labeling (parallel)\"\n",
    "        ))\n",
    "    \n",
    "    # Filter out None results\n",
    "    weak_examples = [r for r in results if r is not None]\n",
    "    \n",
    "    skipped = len(results) - len(weak_examples)\n",
    "    print(f\"✓ Created {len(weak_examples)} weakly-labeled examples\")\n",
    "    print(f\"✗ Skipped {skipped} examples\")\n",
    "    print(f\"  Coverage: {len(weak_examples)/len(df)*100:.1f}%\")\n",
    "    \n",
    "    return Dataset.from_list(weak_examples)\n",
    "\n",
    "\n",
    "# Create weak dataset\n",
    "df_unsup = pd.read_csv(\n",
    "    \"../data/Listing_Titles.tsv\", \n",
    "    sep=\"\\t\", \n",
    "    keep_default_na=False, \n",
    "    na_values=None\n",
    ")\n",
    "\n",
    "weak_ds = create_weak_ner_dataset_parallel(\n",
    "    df_unsup,\n",
    "    gazetteers,\n",
    "    label2id,\n",
    "    sample_size=200000,\n",
    "    n_workers=16  # Adjust based on your CPU\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "537b796d-e393-4f38-8fe5-712ae5eb0139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating weak labels using 16 workers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c33ecf67c74335bd6e4cdad342ccd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Weak labeling (improved):   0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 199284 weakly-labeled examples\n",
      "✗ Skipped 716 examples\n",
      "  Coverage: 99.6%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPROVED WEAK LABELING - Context-Aware Rules\n",
    "# ============================================================================\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm\n",
    "class ImprovedWeakNERLabeler:\n",
    "    \"\"\"\n",
    "    Enhanced weak labeling with:\n",
    "    1. Context-aware rules (für X vs ORIGINAL X)\n",
    "    2. Better pattern matching\n",
    "    3. Position-based hints\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gazetteers: dict, category: int):\n",
    "        self.gazetteers = gazetteers\n",
    "        self.category = category\n",
    "        \n",
    "        # Context indicators for disambiguation\n",
    "        self.context_rules = {\n",
    "            'Kompatible_Fahrzeug_Marke': {\n",
    "                'before': ['für', 'passend', 'geeignet', 'kompatibel'],\n",
    "                'avoid_before': ['original', 'oem', 'genuine']\n",
    "            },\n",
    "            'Hersteller': {\n",
    "                'before': ['original', 'oem', 'genuine', 'von'],\n",
    "                'after': ['marke', 'qualität', 'hersteller']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Enhanced automotive patterns\n",
    "        self.patterns = {\n",
    "            'Anzahl_Der_Einheiten': [\n",
    "                (r'\\b(\\d+)\\s*(stück|stk|x|pcs|piece|teilig)\\b', 1),\n",
    "                (r'\\b(\\d+)er[\\s-]*(set|satz)\\b', 1),\n",
    "                (r'\\b(\\d+)[\\s-]*teilig\\b', 1),\n",
    "            ],\n",
    "            'Einbauposition': [\n",
    "                (r'\\b(vorne?|hinten?|links?|rechts?|va|ha|vl|vr|hl|hr)\\b', 0),\n",
    "                (r'\\b(vorder|hinter|vorn|hint)\\w*\\b', 0),\n",
    "                (r'\\b(front|rear|left|right)\\b', 0),\n",
    "            ],\n",
    "            'Durchmesser': [\n",
    "                (r'\\b(ø|durchmesser|dm\\.?|diameter)\\s*(\\d+)\\s*(mm|cm)?\\b', 2),\n",
    "                (r'\\b(\\d+)\\s*mm\\b', 1),\n",
    "            ],\n",
    "            'Breite': [\n",
    "                (r'\\bbreite\\s*(\\d+)\\s*(mm|cm)?\\b', 1),\n",
    "                (r'\\b(\\d+)\\s*mm\\s*breit\\b', 1),\n",
    "            ],\n",
    "            'Zähnezahl': [\n",
    "                (r'\\b(\\d+)\\s*zähne?\\b', 1),\n",
    "                (r'\\b(\\d+)[\\s-]*teeth\\b', 1),\n",
    "            ],\n",
    "            'Kompatibles_Fahrzeug_Modell': [\n",
    "                # Model patterns: A3, X5, Golf 7, etc.\n",
    "                (r'\\b([A-Z]\\d+)\\b', 0),  # A3, X5, E46\n",
    "                (r'\\b([A-Z]{1,3}[\\s-]\\d{1,3})\\b', 0),  # E-46, C 220\n",
    "            ],\n",
    "        }\n",
    "        \n",
    "        # Common German automotive brands (case-insensitive)\n",
    "        self.known_brands = {\n",
    "            'bmw', 'audi', 'vw', 'volkswagen', 'mercedes', 'benz', 'opel',\n",
    "            'ford', 'renault', 'peugeot', 'citroen', 'fiat', 'seat', 'skoda',\n",
    "            'porsche', 'volvo', 'saab', 'toyota', 'nissan', 'honda', 'mazda',\n",
    "            'hyundai', 'kia', 'chevrolet', 'chrysler', 'jeep', 'land', 'rover',\n",
    "            'mini', 'alfa', 'romeo', 'lancia', 'subaru', 'suzuki', 'mitsubishi',\n",
    "            'dacia', 'jaguar'\n",
    "        }\n",
    "        \n",
    "        # Common part manufacturers\n",
    "        self.known_manufacturers = {\n",
    "            'bosch', 'ate', 'brembo', 'zimmermann', 'febi', 'lemförder',\n",
    "            'sachs', 'bilstein', 'corteco', 'mahle', 'mann', 'hella',\n",
    "            'valeo', 'continental', 'trw', 'skf', 'fag', 'snr', 'ina',\n",
    "            'dayco', 'gates', 'contitech', 'meyle', 'optimal', 'ruville',\n",
    "            'swag', 'topran', 'trucktec', 'vemo', 'pierburg', 'elring'\n",
    "        }\n",
    "        \n",
    "    def _contextual_tagging(self, tokens, labels):\n",
    "            \"\"\"Add context-aware corrections.\"\"\"\n",
    "            for i in range(len(tokens)):\n",
    "                tok_lower = tokens[i].lower()\n",
    "                \n",
    "                # \"für X\" pattern → X is Marke\n",
    "                if i > 0 and tokens[i-1].lower() == 'für':\n",
    "                    if tok_lower in self.known_brands and labels[i] == 'O':\n",
    "                        labels[i] = 'B-Kompatible_Fahrzeug_Marke'\n",
    "                \n",
    "                # \"ORIGINAL X\" pattern → X is Hersteller\n",
    "                if i > 0 and tokens[i-1].lower() == 'original':\n",
    "                    if labels[i] == 'O' or labels[i].endswith('Kompatible_Fahrzeug_Marke'):\n",
    "                        labels[i] = 'B-Hersteller'\n",
    "                \n",
    "                # \"X mm\" pattern → X is dimension\n",
    "                if i < len(tokens) - 1 and tokens[i+1].lower() in ['mm', 'cm']:\n",
    "                    if tokens[i].isdigit() and labels[i] == 'O':\n",
    "                        labels[i] = 'B-Durchmesser'\n",
    "            \n",
    "            return labels\n",
    "    \n",
    "    def label_text(self, text: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Returns list of (token, tag) pairs with context-aware labeling.\n",
    "        \"\"\"\n",
    "        tokens = text.split()\n",
    "        labels = ['O'] * len(tokens)\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # PHASE 1: Gazetteer matching with context awareness\n",
    "        for tag, entities in self.gazetteers.items():\n",
    "            for entity in entities:\n",
    "                entity_lower = entity.lower()\n",
    "                \n",
    "                # Multi-token entities\n",
    "                if ' ' in entity_lower:\n",
    "                    entity_tokens = entity_lower.split()\n",
    "                    for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "                        window = ' '.join(tokens[i:i+len(entity_tokens)]).lower()\n",
    "                        if window == entity_lower:\n",
    "                            # Check context for disambiguation\n",
    "                            resolved_tag = self._resolve_tag_with_context(\n",
    "                                tag, i, tokens, entity_lower\n",
    "                            )\n",
    "                            \n",
    "                            if labels[i] == 'O':  # Don't override\n",
    "                                labels[i] = f'B-{resolved_tag}'\n",
    "                                for j in range(i+1, i+len(entity_tokens)):\n",
    "                                    labels[j] = f'I-{resolved_tag}'\n",
    "                else:\n",
    "                    # Single token\n",
    "                    for i, tok in enumerate(tokens):\n",
    "                        if tok.lower() == entity_lower and labels[i] == 'O':\n",
    "                            resolved_tag = self._resolve_tag_with_context(\n",
    "                                tag, i, tokens, entity_lower\n",
    "                            )\n",
    "                            labels[i] = f'B-{resolved_tag}'\n",
    "        \n",
    "        # PHASE 2: Brand/Manufacturer heuristics\n",
    "        for i, tok in enumerate(tokens):\n",
    "            tok_lower = tok.lower().strip('.,;:')\n",
    "            \n",
    "            if labels[i] == 'O':  # Only if not already labeled\n",
    "                # Check if it's a known brand\n",
    "                if tok_lower in self.known_brands:\n",
    "                    # Context check: für BMW = Marke, ORIGINAL BMW = Hersteller\n",
    "                    if i > 0:\n",
    "                        prev = tokens[i-1].lower()\n",
    "                        if prev in ['für', 'passend', 'geeignet', 'kompatibel']:\n",
    "                            labels[i] = 'B-Kompatible_Fahrzeug_Marke'\n",
    "                        elif prev in ['original', 'oem', 'genuine']:\n",
    "                            labels[i] = 'B-Hersteller'\n",
    "                        else:\n",
    "                            # Default: assume vehicle brand\n",
    "                            labels[i] = 'B-Kompatible_Fahrzeug_Marke'\n",
    "                    else:\n",
    "                        labels[i] = 'B-Kompatible_Fahrzeug_Marke'\n",
    "                \n",
    "                # Check if it's a known manufacturer\n",
    "                elif tok_lower in self.known_manufacturers:\n",
    "                    labels[i] = 'B-Hersteller'\n",
    "        \n",
    "        # PHASE 3: Pattern matching\n",
    "        for tag, patterns in self.patterns.items():\n",
    "            for pattern, group_idx in patterns:\n",
    "                for match in re.finditer(pattern, text_lower, re.IGNORECASE):\n",
    "                    matched_text = match.group(group_idx)\n",
    "                    start_char = match.start(group_idx)\n",
    "                    \n",
    "                    # Map character positions to token indices\n",
    "                    char_pos = 0\n",
    "                    for i, tok in enumerate(tokens):\n",
    "                        tok_len = len(tok)\n",
    "                        if char_pos <= start_char < char_pos + tok_len:\n",
    "                            if labels[i] == 'O':  # Don't override\n",
    "                                labels[i] = f'B-{tag}'\n",
    "                            break\n",
    "                        char_pos += tok_len + 1  # +1 for space\n",
    "        \n",
    "        # PHASE 4: Fix common tokenization issues\n",
    "        labels = self._fix_compound_entities(tokens, labels)\n",
    "        labels = self._contextual_tagging(tokens, labels)\n",
    "        return list(zip(tokens, labels))\n",
    "    \n",
    "    def _resolve_tag_with_context(\n",
    "        self, \n",
    "        original_tag: str, \n",
    "        position: int, \n",
    "        tokens: List[str],\n",
    "        entity: str\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Disambiguate tags using context.\n",
    "        E.g., BMW could be Marke or Hersteller depending on context.\n",
    "        \"\"\"\n",
    "        # Only disambiguate between Marke and Hersteller\n",
    "        if original_tag not in ['Kompatible_Fahrzeug_Marke', 'Hersteller']:\n",
    "            return original_tag\n",
    "        \n",
    "        # Check if entity is a known brand\n",
    "        if entity.lower() not in self.known_brands:\n",
    "            return original_tag\n",
    "        \n",
    "        # Look at previous token\n",
    "        if position > 0:\n",
    "            prev = tokens[position - 1].lower().strip('.,;:')\n",
    "            \n",
    "            # \"für BMW\" → Marke\n",
    "            if prev in ['für', 'passend', 'geeignet', 'kompatibel', 'fits', 'fit']:\n",
    "                return 'Kompatible_Fahrzeug_Marke'\n",
    "            \n",
    "            # \"ORIGINAL BMW\" → Hersteller\n",
    "            if prev in ['original', 'oem', 'genuine', 'von']:\n",
    "                return 'Hersteller'\n",
    "        \n",
    "        # Look at next token\n",
    "        if position < len(tokens) - 1:\n",
    "            next_tok = tokens[position + 1].lower().strip('.,;:')\n",
    "            \n",
    "            # \"BMW Qualität\" → Hersteller\n",
    "            if next_tok in ['qualität', 'original', 'teil', 'hersteller']:\n",
    "                return 'Hersteller'\n",
    "        \n",
    "        # Default: prefer Marke (more common in titles)\n",
    "        return 'Kompatible_Fahrzeug_Marke'\n",
    "    \n",
    "    def _fix_compound_entities(\n",
    "        self, \n",
    "        tokens: List[str], \n",
    "        labels: List[str]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Fix common issues like number+unit being split.\n",
    "        E.g., \"259\" \"mm\" → both should be tagged together\n",
    "        \"\"\"\n",
    "        fixed_labels = labels.copy()\n",
    "        \n",
    "        for i in range(len(tokens) - 1):\n",
    "            current = tokens[i]\n",
    "            next_tok = tokens[i + 1].lower()\n",
    "            \n",
    "            # If current is a number and next is a unit\n",
    "            if current.isdigit() and next_tok in ['mm', 'cm', 'm', 'kg', 'g', 'stk', 'x']:\n",
    "                # If current has a tag and next doesn't\n",
    "                if labels[i].startswith('B-') and labels[i+1] == 'O':\n",
    "                    tag = labels[i][2:]  # Remove B- prefix\n",
    "                    fixed_labels[i+1] = f'I-{tag}'\n",
    "        \n",
    "        return fixed_labels\n",
    "\n",
    "\n",
    "def label_single_example(row_data, gazetteers, label2id, known_brands, known_manufacturers):\n",
    "    \"\"\"Label a single example with improved labeler.\"\"\"\n",
    "    idx, text, category = row_data\n",
    "    \n",
    "    if not text.strip():\n",
    "        return None\n",
    "    \n",
    "    # Use improved labeler\n",
    "    labeler = ImprovedWeakNERLabeler(gazetteers, category)\n",
    "    \n",
    "    try:\n",
    "        token_label_pairs = labeler.label_text(text)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "    tokens = [tok for tok, _ in token_label_pairs]\n",
    "    labels = [label for _, label in token_label_pairs]\n",
    "    \n",
    "    # Validate labels exist in label2id\n",
    "    label_ids = []\n",
    "    for lab in labels:\n",
    "        if lab in label2id:\n",
    "            label_ids.append(label2id[lab])\n",
    "        else:\n",
    "            label_ids.append(label2id['O'])\n",
    "    \n",
    "    # Skip if no entities (after validation)\n",
    "    if all(lid == label2id['O'] for lid in label_ids):\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": label_ids,\n",
    "        \"Category\": category\n",
    "    }\n",
    "\n",
    "def create_weak_ner_dataset_parallel(\n",
    "    unlabeled_df: pd.DataFrame,\n",
    "    gazetteers: dict,\n",
    "    label2id: dict,\n",
    "    sample_size: int = 150000,\n",
    "    n_workers: int = None\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    Parallel version - much faster.\n",
    "    \"\"\"\n",
    "    df = unlabeled_df.sample(n=min(sample_size, len(unlabeled_df)), random_state=42)\n",
    "    \n",
    "    # Prepare data for parallel processing\n",
    "    row_data = [\n",
    "        (idx, str(row.get(\"Title\", \"\")), int(row.get(\"Category\", 1)))\n",
    "        for idx, row in df.iterrows()\n",
    "    ]\n",
    "    \n",
    "    if n_workers is None:\n",
    "        n_workers = max(1, cpu_count() - 2)  # Leave 2 cores free\n",
    "    \n",
    "    print(f\"Generating weak labels using {n_workers} workers...\")\n",
    "\n",
    "    known_brands = {\n",
    "    'bmw', 'audi', 'vw', 'volkswagen', 'mercedes', 'benz', 'opel',\n",
    "    'ford', 'renault', 'peugeot', 'citroen', 'fiat', 'seat', 'skoda'\n",
    "    }\n",
    "        \n",
    "    known_manufacturers = {\n",
    "        'bosch', 'ate', 'brembo', 'zimmermann', 'febi', 'lemförder',\n",
    "        'sachs', 'bilstein', 'corteco', 'mahle', 'mann', 'hella'\n",
    "    }\n",
    "\n",
    "    # Update partial function\n",
    "    label_func = partial(\n",
    "        label_single_example, \n",
    "        gazetteers=gazetteers, \n",
    "        label2id=label2id,\n",
    "        known_brands=known_brands,\n",
    "        known_manufacturers=known_manufacturers\n",
    "    )\n",
    "    \n",
    "    # Rest is same...\n",
    "    with Pool(n_workers) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(label_func, row_data, chunksize=100),\n",
    "            total=len(row_data),\n",
    "            desc=\"Weak labeling (improved)\"\n",
    "        ))\n",
    "    \n",
    "    # Filter out None results\n",
    "    weak_examples = [r for r in results if r is not None]\n",
    "    \n",
    "    skipped = len(results) - len(weak_examples)\n",
    "    print(f\"✓ Created {len(weak_examples)} weakly-labeled examples\")\n",
    "    print(f\"✗ Skipped {skipped} examples\")\n",
    "    print(f\"  Coverage: {len(weak_examples)/len(df)*100:.1f}%\")\n",
    "    \n",
    "    return Dataset.from_list(weak_examples)\n",
    "\n",
    "\n",
    "# Create weak dataset\n",
    "df_unsup = pd.read_csv(\n",
    "    \"../data/Listing_Titles.tsv\", \n",
    "    sep=\"\\t\", \n",
    "    keep_default_na=False, \n",
    "    na_values=None\n",
    ")\n",
    "\n",
    "weak_ds = create_weak_ner_dataset_parallel(\n",
    "    df_unsup,\n",
    "    gazetteers,\n",
    "    label2id,\n",
    "    sample_size=200000,\n",
    "    n_workers=16  # Adjust based on your CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4edb5a8-60ff-4c04-9463-d77a90ed6df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'Category'],\n",
       "        num_rows: 189319\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'Category'],\n",
       "        num_rows: 9965\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_splits = weak_ds.train_test_split(test_size=0.05, seed=42)\n",
    "weak_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c25417eb-c2f2-408d-85b2-f5c3f1c0ca87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weak train: 189319, Weak val: 9965\n"
     ]
    }
   ],
   "source": [
    "# Split into train/val\n",
    "\n",
    "weak_train = weak_splits[\"train\"]\n",
    "weak_val = weak_splits[\"test\"]\n",
    "\n",
    "print(f\"Weak train: {len(weak_train)}, Weak val: {len(weak_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845656fd-0bbd-4e7a-b284-3b71795210e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db48e119ed384f2592d3ef919695d827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1995db69b3ca44d18ad205bca1f98a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f644f3a76a5475681a86d1c01a8ec6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f0b83a3b254462b94a74da0bdf099c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/189319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ff637613d7475181a151119b59e5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/9965 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ca35dc1b4a4d2b958e2f65a6e77d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36663296cc5749b69e82a1974b7b2651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting weak NER pre-training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4440' max='4440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4440/4440 41:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.262800</td>\n",
       "      <td>0.053387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.024563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.071600</td>\n",
       "      <td>0.015878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.011402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.008833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.007253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.007079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.005948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/shehryarkhan/software-testing-llm/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weak NER encoder saved!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# WEAK NER PRE-TRAINING (Replaces MLM Step 1)\n",
    "# ============================================================================\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, PreTrainedModel, AutoModel\n",
    "\n",
    "base_model = \"microsoft/deberta-v3-large\"\n",
    "tok = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "\n",
    "\n",
    "def tok_fn_weak(batch):\n",
    "    \"\"\"Tokenize for NER.\"\"\"\n",
    "    enc = tok(\n",
    "        batch[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        # padding=False,\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "    all_labels = []\n",
    "    for i in range(len(enc[\"input_ids\"])):\n",
    "        word_ids = enc.word_ids(batch_index=i)\n",
    "        gold = batch[\"ner_tags\"][i]\n",
    "        seq = []\n",
    "        prev = None\n",
    "        \n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                seq.append(-100)\n",
    "            elif wid != prev:\n",
    "                seq.append(gold[wid])\n",
    "                prev = wid\n",
    "            else:\n",
    "                seq.append(-100)\n",
    "        \n",
    "        all_labels.append(seq)\n",
    "    \n",
    "    enc[\"labels\"] = all_labels\n",
    "    enc[\"category_id\"] = batch[\"Category\"]\n",
    "    return enc\n",
    "\n",
    "\n",
    "# Tokenize weak datasets\n",
    "tok_weak_train = weak_train.map(\n",
    "    tok_fn_weak, \n",
    "    batched=True, \n",
    "    remove_columns=[\"tokens\", \"ner_tags\", \"Category\"],\n",
    "    num_proc=16\n",
    ")\n",
    "\n",
    "tok_weak_val = weak_val.map(\n",
    "    tok_fn_weak, \n",
    "    batched=True, \n",
    "    remove_columns=[\"tokens\", \"ner_tags\", \"Category\"],\n",
    "    num_proc=16\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SIMPLE TOKEN CLASSIFICATION MODEL (No CRF yet)\n",
    "# ============================================================================\n",
    "\n",
    "class WeakNERModel(PreTrainedModel):\n",
    "    \"\"\"Simple token classification for weak pre-training.\"\"\"\n",
    "    config_class = AutoConfig\n",
    "    \n",
    "    def __init__(self, config, num_labels=None, base_model_name=None):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Load base encoder\n",
    "        if base_model_name:\n",
    "            self.encoder = AutoModel.from_pretrained(\n",
    "                base_model_name, \n",
    "            )\n",
    "        else:\n",
    "            self.encoder = AutoModel.from_config(config)\n",
    "        \n",
    "        # Simple classifier (no CRF for weak training)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        \n",
    "        # Initialize classifier only\n",
    "        nn.init.normal_(self.classifier.weight, std=0.02)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        outputs = self.encoder(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        logits = self.classifier(self.dropout(sequence_output))\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                logits.view(-1, self.num_labels),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "cfg = AutoConfig.from_pretrained(\n",
    "    base_model,\n",
    "    num_labels=len(label_list)\n",
    ")\n",
    "\n",
    "weak_model = WeakNERModel(\n",
    "    cfg,\n",
    "    num_labels=len(label_list),\n",
    "    base_model_name=base_model\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "# Simple data collator\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tok,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# WEAK TRAINING ARGUMENTS (Conservative)\n",
    "# ============================================================================\n",
    "\n",
    "weak_args = TrainingArguments(\n",
    "    output_dir=\"../models/deberta-improved-weak-ner-mk-2\",\n",
    "    \n",
    "    # Batch size\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=48,\n",
    "    gradient_accumulation_steps=4,  \n",
    "    # Learning rate (higher than fine-tuning, lower than from scratch)\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Epochs (2-3 for weak labels)\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Mixed precision\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # Efficiency\n",
    "    dataloader_num_workers=16,\n",
    "    dataloader_pin_memory=True,\n",
    "    gradient_checkpointing=False,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=100,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "weak_trainer = Trainer(\n",
    "    model=weak_model,\n",
    "    args=weak_args,\n",
    "    train_dataset=tok_weak_train,\n",
    "    eval_dataset=tok_weak_val,\n",
    "    data_collator=collator,\n",
    "    processing_class=tok,\n",
    ")\n",
    "\n",
    "# Train on weak labels\n",
    "print(\"Starting weak NER pre-training...\")\n",
    "weak_trainer.train()\n",
    "\n",
    "# Save encoder only (discard classifier head)\n",
    "weak_model.encoder.save_pretrained(\"../models//deberta-improved-weak-ner-mk-2\")\n",
    "tok.save_pretrained(\"../models//deberta-improved-weak-ner-mk-2\")\n",
    "print(\"Weak NER encoder saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM-TESTING)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
